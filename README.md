# ETL Transaction Data Pipeline (Apache Airflow)

This project contains an automated ETL pipeline built using **Apache Airflow**.  
The pipeline extracts data from CSV and PostgreSQL sources, transforms it, and loads it into the final PostgreSQL database for analytics.

### ğŸ“ Project Structure
```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_transaction_data_dag.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”œâ”€â”€ transform_data.py
â”‚   â””â”€â”€ load_data.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   - (input_data.csv)
â”‚   â””â”€â”€ output/
â”‚       - (output_data.csv)
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
### ğŸ“ Dependencies Required
```
apache-airflow==2.7.1
apache-airflow-providers-postgres

pandas
pendulum

psycopg2-binary
SQLAlchemy
```


### ğŸ”„ Pipeline Overview

![alt text](img-pipeline-overview.png)

### **Extract**
- Reads 5 CSV files as raw input.
- Converts 2 CSV tables (`dummy_transaksi_bus`, `dummy_transaksi_halte`) into PostgreSQL tables.
- Produces 3 CSV-based sources and 2 PostgreSQL-based sources.

### **Transform**
- Cleans and preprocesses all extracted datasets.
- Applies data quality rules and transformations.
- Generates unified, analytics-ready dataset.

### **Load**
- Exports the final transformed tables into:

    - PostgreSQL (fact & aggregations tables)

    - CSV report files in ```/data/output/```


### â° Scheduling

The pipeline runs **every day at 07:00 (Asia/Jakarta)**.


```
schedule_interval="0 7 * * *"
```

### ğŸ³ Running with Docker
#### 1. Clone the Repository
```
git clone https://github.com/widiarsaf/airflow-etl-pipeline-widiareta.git
cd airflow-etl-pipeline-widiareta
```

#### 2. Setup and Start Airflow
- Clean any old resources (optional but recommended)
```
docker compose down --volumes --remove-orphans
```

- Build and Initialize Airflow
```
docker compose build
docker compose run airflow-init
```

- Start Airflow
```
docker-compose up -d
```

#### 3. Access Airflow Web UI 
http://localhost:8080

Login (default):
- **Username:** admin  
- **Password:** admin  

#### 4. Setting Up PostgreSQL Connection in Airflow
To enable the ETL pipeline to communicate with the PostgreSQL database, you need to configure a connection in the Airflow UI. 

- Open Airflow Web UI.

- Navigate to Admin â†’ Connections.

- Search for â€œpostgres_defaultâ€ connection.

- Fill in the following details:

| Field         | Value              |
| ------------- | ------------------ |
| **Conn Id**   | `postgres_default` |
| **Conn Type** | Postgres           |
| **Host**      | `postgres`         |
| **Schema**    | `airflow`          |
| **Login**     | `airflow`          |
| **Password**  | `airflow`          |
| **Port**      | `5432`             |
| **Extra**     | `(leave completely empty)`    |

- For connection setup, please remove `{}` in Extra column, leave completely empty. This is important step to make sure airflow can connect to postgres database.

    - âœ”ï¸ Correct: (leave blank)
    - âŒ Incorrect: {} or any JSON content
![alt text](img-pg-conn.png)

- Click ***Save*** button.

ğŸ“Œ Note:

- The host postgres comes from ```docker-compose.yml```.

- The DAG expects the connection ID ```postgres_default```.



#### 5. Start the Pipeline
![alt text](img-start-etl-pipeline.png)

Trigger the DAG using the Play (â–º) button in Airflow.
Airflow will run all tasks in sequenceâ€”from ```start``` â†’ CSV loading â†’ extraction â†’ transformation â†’ data loading â†’ ```end```.
Each step follows task dependencies automatically.

